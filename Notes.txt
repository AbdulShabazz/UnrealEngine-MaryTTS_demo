FILES
- .JAR files are OS portable JAVA runtime executables
- .JAVA files are sourcecode access files for most of the APIs
- .VOICE files contain lang-specific phones-, accent-
- .GROOVY files (cf. .bat files)
- .GRADLE files (cf. cmake)
- .WAV files are (hgh fidelilty lossless) audio files (Windows)
- .FLAC files are (hgh fidelilty lossless) audio files (Mac)
- .MP3 are platform independant (compresssed lossy) audio files
- .code-workspace is an optional workspace viewer for Visual Studio Code

Speech Synthesis 1

Center frequencies (typical)

F1 50 hz
F2 80 hz
F3 2.5 khz
F4 3 khz

Incorporate a spectrograph (spectrogram generator)-, interpreter- / analyzer-, and playback engine- to intuitively guide wave synthesis . 
A wide-band spectrogram (eg. 50 db) reveals nuanced consonant harmonic structure . A narrow-band spectrogram (eg. 30 db) minimizes weak sounds however the harmonic structure of vowels remains visible . 
The useful bands for speech analysis lies between 25 hz - 400 hz .

Max sample rate : 10 ms 
Harmonic range: 50 hz

One wave synthesis solution is to store typical waveform shapes for use as typical waveform samples which can be recalibrated around the desired fundamental frequency. 

Fricative models seem inconsequential in high-fidelity speech synthesis systems. 

Plosives can be modeled using an (resonator)  impulse (response) in conjunction with a (convolution) wave-shaping filter to achieve the desired spectrum shape.

Using models such as lumped constant transmission lines with realistic losses in each section can aid in speech synthesis (cascaded model)

Parallel formant synthesis can be more performant however the signals must be added carefully so as not to skew the poles outside the range of human voice. 
A suggestion is to find a common denominator between formants and scale numerators accordingly. If all formants are given the same spectrum dimension their result equals a Cascaded model!

Each formant usually employs a resonator and multiple resonators can either be constructed in series (cascaded) or in parallel. Benefits of series: a single amplitude, 
determined by their frequencies, damping factors or bandwidths; non-nasal vowels modeling. 

Plosives, Fricatives, and Nasals need to be modeled independently and accurately using the resonator formants, F0-F4.

The schwa phoneme closely tracks the fundamental frequency of the human voice.

A first proposed model, intended for offline speech synthesis.

input parameters for statements/phrases/words/phonemes/glyph-combinations, glyphs : gain, amplitude, gender: (fundamental frequency slider, F0, F1,...,Fn), Prosody Quality (Smooth, Harsh) governed by Pitch, 
Duration, and Intensity.

Verify MD5 (MD5.voice) before opening target voice file. (optional)

NoiseGPT (Content Creation Tool)

Methods: Deep Learning Models, Concatenative Speech, Statistical Parametric Methods

Note: Rather than employing a filter for each phone, instead employ a specialized transition filter which generates partial phone pairs which match at optimal zero-crossings to support concatenation 
(eg. "apple ":null-a,a-p,p-p,p-l,l-null)(ie.diphones; cf.triphones)

Speech Synthesis 2

The best strategy to synthesize formants so that their inherent triggers are event-based and not time-synchronous based to generate speech signals of arbitrary length is to use a **parametric formant synthesizer**. 
Parametric formant synthesizers use a set of parameters to control the formant frequencies of the synthesized speech. These parameters can be triggered by events, 
such as the onset of a phoneme or the release of a consonant.

One way to implement a parametric formant synthesizer is to use a **source-filter model** of speech production. In this model, 
the speech signal is generated by a source signal (such as a white noise signal) that is passed through a filter (the vocal tract). 
The formant frequencies of the speech signal are determined by the characteristics of the vocal tract filter.

To synthesize formants using a source-filter model, we can use a set of parameters to control the characteristics of the vocal tract filter. 
For example, we can use one parameter to control the bandwidth of each formant. We can use another parameter to control the frequency of each formant. 
We can then trigger these parameters by events, such as the onset of a phoneme or the release of a consonant.

Another way to implement a parametric formant synthesizer is to use a **cascade of resonators**. In this approach, we use a set of cascaded resonators to model the formant frequencies of the speech signal. 
Each resonator is tuned to a specific frequency. The output of each resonator is fed into the next resonator. The overall output of the cascade of resonators is the synthesized speech signal.

To synthesize formants using a cascade of resonators, we can use a set of parameters to control the frequency and bandwidth of each resonator. We can then trigger these parameters by events, 
such as the onset of a phoneme or the release of a consonant.

One advantage of using a parametric formant synthesizer is that it can be used to generate speech signals of arbitrary length. This is because the formant frequencies of the synthesized speech are triggered by events, 
and not by a time-synchronous clock. This makes parametric formant synthesizers well-suited for synthesizing speech for text-to-speech systems and speech recognition systems.

Another advantage of using a parametric formant synthesizer is that it can be used to generate speech signals with different formant frequency distributions. 
This is because the formant frequencies of the synthesized speech are controlled by parameters. This makes parametric formant synthesizers well-suited for synthesizing speech 
in different languages and for synthesizing speech with different speaker characteristics.

Here are some examples of how event-based formant synthesis can be used to generate speech signals of arbitrary length:

* **Synthesizing a sentence:** To synthesize a sentence, we can use a parametric formant synthesizer to generate a sequence of formant sequences. Each formant sequence would represent a phoneme in the sentence. 
The onset of each formant sequence would be triggered by the onset of the corresponding phoneme. The release of each formant sequence would be triggered by the release of the corresponding phoneme.
* **Synthesizing a word:** To synthesize a word, we can use a parametric formant synthesizer to generate a formant sequence. 
The formant sequence would represent the phonemes in the word. The onset of the formant sequence would be triggered by the onset of the first phoneme in the word. 
The release of the formant sequence would be triggered by the release of the last phoneme in the word.
* **Synthesizing a syllable:** To synthesize a syllable, we can use a parametric formant synthesizer to generate a formant sequence. The formant sequence would represent the phonemes in the syllable. 
The onset of the formant sequence would be triggered by the onset of the first phoneme in the syllable. The release of the formant sequence would be triggered by the release of the last phoneme in the syllable.
* ***Phoneme transitions:** This speech synthesis library my require a powerful signal processing framework to synthesize phoneme transitions. 
Examine, for example, the transition from the phoneme /s/ to the phoneme /t/, the phoneme /s/ is a fricative phoneme and the phoneme /t/ is a plosive phoneme. 
So the fricative, the plosive, and the transition waveforms would need to be adjusted or altered.

Synthesizing a sentence

A first strategy may be to use a single for-loop which operates each time step for the entire duration of the sentence.

A piece-wise defined sine wave generates the fundamental- and formant- frequencies of the entire sentence in parallel.

Example.


```python
Voice = 
VocalTractNN(Phoneme['/h/'], Prosody, Emotive).
VocalTractNN(Phoneme['/schwa/'], Prosody, Emotive).
VocalTractNN(Phoneme['/l/'], Prosody, Emotive).
VocalTractNN(Phoneme['/o/'], Prosody, Emotive).
VocalTractNN(Phoneme['/w/'], Prosody, Emotive).
VocalTractNN(Phoneme['/ur/'], Prosody, Emotive).
VocalTractNN(Phoneme['/l/'], Prosody, Emotive).
VocalTractNN(Phoneme['/d/'], Prosody, Emotive).Voice;
```


```python
class VocalTractNN(self, Phoneme, Prosody, Emotive):
    # Phoneme is the phoneme to be synthesized
    # Prosody is the prosody of the sentence
    # Emotive is the emotional tone of the sentence
    # VocalTractNN is the neural network trained to synthesize the phoneme
    # Voice is the synthesized speech signal

    for t in range(0,DURATION): # DURATION is the duration of the sentence in seconds
        for i in range(0,NUM_FORMANTS): # NUM_FORMANTS is the number of formants in the sentence
            f[i] = f[i] + df[i] # df[i] is the change in frequency of the ith formant
            a[i] = a[i] + da[i] # da[i] is the change in amplitude of the ith formant
            s[i] = a[i] * sin(2*pi*f[i]*t) # s[i] is the ith formant signal and s is the pre-synthesized speech signal

    for t in range(0,DURATION): # DURATION is the duration of the sentence in seconds
        voice_[t] = NN(s[t], prosody, emotional_tone) # NN is the neural network trained to add shimmer, jitter, noise gates, and natural details via signal processing to the pre-synthesized speech signal

    voice = sum(voice_) # voice is the synthesized speech signal

    return self
```

Overall, event-based formant synthesis is a prefered personal method for generating speech signals of arbitrary length. It can be used to synthesize speech for a variety of applications, 
including text-to-speech systems, speech recognition systems, and speech coding systems.

Speech Synthesis 3

Support voice aggregation (chorus), Mechanical 
voices, and other forms if altered voices.

Voice Synthesis can use statistical based signal generation (learned) for the base components and shapes and then rules-based signal generation for the remaining support frequencies in the spectrum.

American English Hello World IPA spelling

    "Hello": /həˈloʊ/
        /h/ : 'h' (ASCII: 0x68)
        /ə/ : 'ə' (UTF-8 Hex: 0xC9 0x99)
        /ˈl/ : 'l' (ASCII: 0x6C)
        /oʊ/ : 'o' (ASCII: 0x6F) + 'ʊ' (UTF-8 Hex: 0xCA 0x8A)
    "World": /wɝld/
        /w/ : 'w' (ASCII: 0x77)
        /ɝ:/ : 'ɝ:' (UTF-8 Hex: 0xC9 0x9D)
        /l/ : 'l' (ASCII: 0x6C)
        /d/ : 'd' (ASCII: 0x64)
